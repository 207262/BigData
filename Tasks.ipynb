{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Init Spark</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "conf = SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, trim, sort_array, collect_list, explode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define data path</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = 'Set your path here'\n",
    "followers = os.path.join(data_dir, 'followers.parquet')\n",
    "followers_posts = os.path.join(data_dir, 'followers_posts_api_final.json')\n",
    "followers_posts_likes = os.path.join(data_dir, 'followers_posts_likes.parquet')\n",
    "posts = os.path.join(data_dir, 'posts_api.json')\n",
    "posts_likes = os.path.join(data_dir, 'posts_likes.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = sqlContext.read.json(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_likes_df = spark.read.load(posts_likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_df = spark.read.load(followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_posts_df = sqlContext.read.json(followers_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_posts_likes_df = spark.read.load(followers_posts_likes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task 1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get top posts</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = posts_df.na.drop(subset=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'task1_posts'\n",
    "\n",
    "column = 'likes'\n",
    "answer = posts_df.orderBy(posts_df[column].desc(), asc=False).limit(20).toPandas()\n",
    "answer_json = answer.to_json()\n",
    "with open('{}_{}.json'.format(prefix, column), 'w') as f:\n",
    "    f.write(answer_json)\n",
    "    \n",
    "column = 'reposts'\n",
    "answer = posts_df.orderBy(posts_df[column].desc(), asc=False).limit(20).toPandas()\n",
    "answer_json = answer.to_json()\n",
    "with open('{}_{}.json'.format(prefix, column), 'w') as f:\n",
    "    f.write(answer_json)\n",
    "    \n",
    "column = 'comments'\n",
    "answer = posts_df.orderBy(posts_df[column].desc(), asc=False).limit(20).toPandas()\n",
    "answer_json = answer.to_json()\n",
    "with open('{}_{}.json'.format(prefix, column), 'w') as f:\n",
    "    f.write(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'task1_followers'\n",
    "\n",
    "column = 'likes'\n",
    "answer = followers_posts_df.orderBy(followers_posts_df[column].desc(), asc=False).limit(20).toPandas()\n",
    "answer_json = answer.to_json()\n",
    "with open('{}_{}.json'.format(prefix, column), 'w') as f:\n",
    "    f.write(answer_json)\n",
    "    \n",
    "column = 'reposts'\n",
    "answer = followers_posts_df.orderBy(followers_posts_df[column].desc(), asc=False).limit(20).toPandas()\n",
    "answer_json = answer.to_json()\n",
    "with open('{}_{}.json'.format(prefix, column), 'w') as f:\n",
    "    f.write(answer_json)\n",
    "    \n",
    "column = 'comments'\n",
    "answer = followers_posts_df.orderBy(followers_posts_df[column].desc(), asc=False).limit(20).toPandas()\n",
    "answer_json = answer.to_json()\n",
    "with open('{}_{}.json'.format(prefix, column), 'w') as f:\n",
    "    f.write(answer_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task 2</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get top followers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_count = posts_likes_df.groupby('likerId').count()\n",
    "answer = likes_count.orderBy(col('count').desc(), asc=False).limit(20)\n",
    "answer_json = answer.toPandas().to_json()\n",
    "with open('task2_by_likes.json', 'w') as f:\n",
    "    f.write(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = followers_posts_df.withColumn('copy_history', explode('copy_history'))\n",
    "answer = df_exploded.filter(df_exploded.copy_history.owner_id == '-94').groupby('owner_id').count().sort('count', ascending=False).limit(20)\n",
    "answer_json = answer.toPandas().to_json()\n",
    "with open('task2_by_reposts.json', 'w') as f:\n",
    "    f.write(answer_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task 3</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get posts and reposts</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = followers_posts_df.withColumn('copy_history', explode('copy_history'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = df_exploded.filter(df_exploded.copy_history.owner_id == '-94')\n",
    "selected = filtered.select(filtered.copy_history.id.alias(\"group_post_id\"), filtered.id.alias(\"user_post_id\"))\n",
    "selected = selected.sort('copy_history.id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = selected.groupby(\"group_post_id\").agg(collect_list(\"user_post_id\")).sort('group_post_id')\n",
    "answer_json = answer.toPandas().to_json()\n",
    "with open('task3_answer.json', 'w') as f:\n",
    "    f.write(answer_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task 4</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load emojis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "import pandas as pd\n",
    "\n",
    "emojis_df = pd.read_csv('emoji_sentiment.csv')\n",
    "emojis_df['Sentiment'] = emojis_df[['Negative', 'Neutral', 'Positive']].idxmax(axis=1)\n",
    "emojis_df['unicode_emoji'] = [chr(int(emoji, 16)) for emoji in emojis_df['Unicode codepoint'].tolist()]\n",
    "positive_emojis = emojis_df[emojis_df['Sentiment'] == 'Positive']['unicode_emoji'].values.tolist()\n",
    "negative_emojis = emojis_df[emojis_df['Sentiment'] == 'Negative']['unicode_emoji'].values.tolist()\n",
    "\n",
    "unicode_positive_emojis = {}\n",
    "unicode_negative_emojis = {}\n",
    "unicode_neutral_emojis = {}\n",
    "\n",
    "for key, value in emoji.UNICODE_EMOJI.items():\n",
    "    if key in positive_emojis:\n",
    "        unicode_positive_emojis[key] = value\n",
    "    elif key in negative_emojis:\n",
    "        unicode_negative_emojis[key] = value\n",
    "    else:\n",
    "        unicode_neutral_emojis[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extract emojis without brodcasing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_positive_emojis(_str):\n",
    "    return ''.join(c for c in _str if c in unicode_positive_emojis)\n",
    "\n",
    "def extract_negative_emojis(_str):\n",
    "    return ''.join(c for c in _str if c in unicode_negative_emojis)\n",
    "\n",
    "def extract_neutral_emojis(_str):\n",
    "    return ''.join(c for c in _str if c in unicode_neutral_emojis)\n",
    "\n",
    "extract_positive_emojis_udf = udf(lambda _str: extract_positive_emojis(_str), StringType())\n",
    "extract_negative_emojis_udf = udf(lambda _str: extract_negative_emojis(_str), StringType())\n",
    "extract_neutral_emojis_udf = udf(lambda _str: extract_neutral_emojis(_str), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extract emojis with brodcasing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emojis(_str, emojis_list):\n",
    "    return ''.join(c for c in _str if c in emojis_list)\n",
    "\n",
    "def udf_extract_emojis(emojis_list):\n",
    "    return udf(lambda x : extract_emojis(x, emojis_list))\n",
    "\n",
    "\n",
    "b_positive_emojis = sc.broadcast(sc.parallelize(unicode_positive_emojis).collect())\n",
    "b_negative_emojis = sc.broadcast(sc.parallelize(unicode_negative_emojis).collect())\n",
    "b_neutral_emojis = sc.broadcast(sc.parallelize(unicode_neutral_emojis).collect())\n",
    "b_emojis = sc.broadcast(sc.parallelize(emoji.UNICODE_EMOJI).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Process posts</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------------+---------------+--------------+\n",
      "|   id|                text|positive_emojis|negative_emojis|neutral_emojis|\n",
      "+-----+--------------------+---------------+---------------+--------------+\n",
      "|17273|[–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ ...|               |               |              |\n",
      "|17301|–î–æ—Ä–æ–≥–∏–µ –¥—Ä—É–∑—å—è, –º...|               |               |              |\n",
      "|17295|[–ü–µ—Ä–≤—ã–π –≤—ã–ø—É—Å–∫ –ú–æ...|               |               |              |\n",
      "|17293|[–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ò—à–µ–≤—Å–∫...|               |               |              |\n",
      "|17277|–î–ª—è –∂–µ–ª–∞—é—â–∏—Ö —É–≤–∏–¥...|               |               |              |\n",
      "|17276|[–ü—Ä–æ—Ñ–µ—Å—Å–æ—Ä—ã –ü–∞—É–ª—å...|               |               |              |\n",
      "|17326|–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –ø—Ä–∞–∑...|               |               |              |\n",
      "|17323|[–¢–∞—Ç—å—è–Ω–∞ –ë–æ–≥–æ–º–∞–∑–æ...|               |               |              |\n",
      "|17322|[–£ –µ–≤—Ä–æ–ø–µ–π—Å–∫–æ–π –∏ ...|               |               |              |\n",
      "|17365|[¬´–†–æ–∂–¥–µ—Å—Ç–≤–µ–Ω—Å–∫–∏–µ ...|               |               |              |\n",
      "|17364|[–ò—Ç–æ–≥–∏ –ø–µ—Ä–≤–æ–π —Å–µ—Å...|               |               |              |\n",
      "|17363|[–í –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä...|               |               |              |\n",
      "|17381|üéÑ–£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç –ò–¢–ú...|           üéÑüéÑ|               |              |\n",
      "|17379|–°–ø–∞—Å–∏–±–æ –ú–∞—Ä–∏–∏ –∑–∞ ...|               |               |              |\n",
      "|17376|–î–æ—Ä–æ–≥–∏–µ —Å—Ç—É–¥–µ–Ω—Ç—ã ...|               |               |              |\n",
      "|17369|[–¶–µ–Ω—Ç—Ä —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π...|               |               |              |\n",
      "|17368|[–ß–ª–µ–Ω—ã –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥...|               |               |              |\n",
      "|17382|[–£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—ã –º–æ–≥...|               |               |              |\n",
      "| 4979|–Ø, –≤—Ä–æ–¥–µ, –±–∞–ª—Ç–∏–∫—É...|               |               |              |\n",
      "| 4977|–ê –±—Ä–∞—Ç—å –≤–∏–∑—É –ª—É—á—à...|               |               |              |\n",
      "+-----+--------------------+---------------+---------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_df = posts_df.filter(posts_df.text != '')\n",
    "processed_df = processed_df.withColumn('positive_emojis', udf_extract_emojis(b_positive_emojis.value)(col('text')))\n",
    "processed_df = processed_df.withColumn('negative_emojis', udf_extract_emojis(b_negative_emojis.value)(col('text')))\n",
    "processed_df = processed_df.withColumn('neutral_emojis', udf_extract_emojis(b_neutral_emojis.value)(col('text')))\n",
    "processed_df = processed_df.select('id', 'text', 'positive_emojis', 'negative_emojis', 'neutral_emojis')\n",
    "processed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "739"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_df = processed_df.filter((processed_df.positive_emojis != '') | (processed_df.negative_emojis != '') | (processed_df.neutral_emojis != ''))\n",
    "answer_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1027.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 47, DESKTOP-P9LJEH2, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.0.0-preview2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 577, in main\n  File \"C:\\spark-3.0.0-preview2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 835, in read_int\n    length = stream.read(4)\n  File \"C:\\Users\\mstrHW\\Miniconda3\\lib\\socket.py\", line 589, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:484)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:726)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:321)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:441)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:444)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1989)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1977)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1976)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1976)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:956)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:956)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:956)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2155)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2144)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:758)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2137)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2156)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:431)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3482)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2581)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3472)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$4(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3468)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2581)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2788)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:297)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:334)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.0.0-preview2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 577, in main\n  File \"C:\\spark-3.0.0-preview2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 835, in read_int\n    length = stream.read(4)\n  File \"C:\\Users\\mstrHW\\Miniconda3\\lib\\socket.py\", line 589, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:484)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:726)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:321)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:441)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:444)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-c0a8839ae666>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0manswer_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-3.0.0-preview2-bin-hadoop3.2\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \"\"\"\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.0-preview2-bin-hadoop3.2\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1286\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.0-preview2-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.0-preview2-bin-hadoop3.2\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1027.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 47, DESKTOP-P9LJEH2, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.0.0-preview2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 577, in main\n  File \"C:\\spark-3.0.0-preview2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 835, in read_int\n    length = stream.read(4)\n  File \"C:\\Users\\mstrHW\\Miniconda3\\lib\\socket.py\", line 589, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:484)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:726)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:321)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:441)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:444)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1989)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1977)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1976)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1976)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:956)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:956)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:956)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2155)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2144)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:758)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2137)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2156)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:431)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3482)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2581)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3472)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$4(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3468)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2581)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2788)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:297)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:334)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.0.0-preview2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 577, in main\n  File \"C:\\spark-3.0.0-preview2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 835, in read_int\n    length = stream.read(4)\n  File \"C:\\Users\\mstrHW\\Miniconda3\\lib\\socket.py\", line 589, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:484)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:726)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:321)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:441)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:444)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "answer_json = answer_df.toPandas().to_json()\n",
    "with open('task4_answer.json', 'w') as f:\n",
    "    f.write(answer_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task 5</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Filter self-likes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_posts_likes_df = followers_posts_likes_df.select(\n",
    "    followers_posts_likes_df.ownerId,\n",
    "    followers_posts_likes_df.likerId\n",
    ").where(followers_posts_likes_df.ownerId != followers_posts_likes_df.likerId).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get friends ids</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_1 = followers_posts_likes_df.alias('users_1')\n",
    "users_2 = followers_posts_likes_df.alias('users_2')\n",
    "\n",
    "friends_data = users_1.join(users_2,\n",
    "    (col('users_1.ownerId') == col('users_2.likerId')) &\n",
    "    (col('users_1.likerId') == col('users_2.ownerId'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as pyspark_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_data = friends_data.select(\n",
    "    col('users_1.ownerId').alias('user'),\n",
    "    col('users_2.ownerId').alias('friends')\n",
    ").groupby('user').agg(pyspark_functions.collect_list('friends').alias('friends'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "| user|             friends|\n",
      "+-----+--------------------+\n",
      "|  637|[1567, 94494, 815...|\n",
      "| 1119|           [2004962]|\n",
      "| 1127|[27857, 317799, 2...|\n",
      "| 1174|[2134327, 139499389]|\n",
      "| 1567|  [637, 2212, 94494]|\n",
      "| 2212|              [1567]|\n",
      "| 4023|[1548876, 1034920...|\n",
      "| 7373|[180092, 317799, ...|\n",
      "| 8909|   [27905, 12742533]|\n",
      "|12671|[12977, 234753, 3...|\n",
      "|12977|[12671, 269559, 3...|\n",
      "|15221|[25554, 50601, 36...|\n",
      "|18589|             [18751]|\n",
      "|18751|             [18589]|\n",
      "|18994|             [45781]|\n",
      "|20972|             [29840]|\n",
      "|21571|            [410199]|\n",
      "|22304|[27857, 507824, 3...|\n",
      "|24147|[27419, 81102, 42...|\n",
      "|24770|           [4656597]|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friends_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_json = friends_data.toPandas().to_json()\n",
    "with open('task5_answer.json', 'w') as f:\n",
    "    f.write(answer_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task 6</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get friends ids</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_1 = followers_posts_likes_df.alias('users_1')\n",
    "users_2 = followers_posts_likes_df.alias('users_2')\n",
    "\n",
    "friends_data = users_1.join(users_2,\n",
    "    (col('users_1.ownerId') == col('users_2.likerId')) &\n",
    "    (col('users_1.likerId') == col('users_2.ownerId')),\n",
    "    'left_outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_data = friends_data.select(\n",
    "    col('users_1.ownerId').alias('user'),\n",
    "    col('users_1.likerId').alias('fan')\n",
    ").groupby('user').agg(pyspark_functions.collect_list('fan').alias('fans'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|user|                fans|\n",
      "+----+--------------------+\n",
      "| 637|[143, 254, 450, 4...|\n",
      "|1087|[662, 1818, 4177,...|\n",
      "|1119|[1099, 4836, 2004...|\n",
      "|1127|[1567, 1679, 2282...|\n",
      "|1174|[7627, 19755, 272...|\n",
      "|1567|[637, 655, 771, 1...|\n",
      "|1632|[42940, 68535, 75...|\n",
      "|1669|[1017, 1216, 6107...|\n",
      "|2212|[1099, 1405, 1567...|\n",
      "|2718|[1105, 15275, 409...|\n",
      "|2976|[2541, 3448, 5304...|\n",
      "|3420|[14, 17, 197, 243...|\n",
      "|3768|[25068229, 386898...|\n",
      "|3868|[19249, 23678, 26...|\n",
      "|3972|[582, 1410, 34660...|\n",
      "|4023|[4734, 21272, 283...|\n",
      "|4107|[6707, 7439, 3349...|\n",
      "|4990|[277835, 286893, ...|\n",
      "|5630|[105815, 7696061,...|\n",
      "|5648|[1614, 1991, 3757...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friends_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_json = friends_data.toPandas().to_json()\n",
    "with open('task6_answer.json', 'w') as f:\n",
    "    f.write(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
